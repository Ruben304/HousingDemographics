{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoGluon Discrete MinMaxNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.model_selection import train_test_split\n",
    "discreteDf = TabularDataset('../discrete_minmaxnorm_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    187594.000000\n",
       "mean          0.037885\n",
       "std           0.021942\n",
       "min           0.000000\n",
       "25%           0.033898\n",
       "50%           0.036268\n",
       "75%           0.039685\n",
       "max           1.000000\n",
       "Name: GrowthRate, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(discreteDf, test_size=0.25, random_state=0)\n",
    "label = 'GrowthRate'\n",
    "train_data[label].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20240429_193316\"\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n",
      "\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n",
      "\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n",
      "\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20240429_193316\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.11.6\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   Darwin Kernel Version 23.0.0: Fri Sep 15 14:42:42 PDT 2023; root:xnu-10002.1.13~1/RELEASE_X86_64\n",
      "CPU Count:          8\n",
      "Memory Avail:       5.79 GB / 16.00 GB (36.2%)\n",
      "Disk Space Avail:   6.28 GB / 371.60 GB (1.7%)\n",
      "\tWARNING: Available disk space is low and there is a risk that AutoGluon will run out of disk during fit, causing an exception. \n",
      "\tWe recommend a minimum available disk space of 10 GB, and large datasets may require more.\n",
      "===================================================\n",
      "Train Data Rows:    187594\n",
      "Train Data Columns: 6\n",
      "Label Column:       GrowthRate\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (1.0, 0.0, 0.03789, 0.02194)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5922.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.59 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 6 | ['WHITE', 'BLACK_OR_AFRICAN_AMERICAN', 'AMERICAN_INDIAN_AND_ALASKA_NATIVE', 'ASIAN', 'NATIVE_HAWAIIAN_AND_OTHER_PACIFIC_ISLANDER', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 6 | ['WHITE', 'BLACK_OR_AFRICAN_AMERICAN', 'AMERICAN_INDIAN_AND_ALASKA_NATIVE', 'ASIAN', 'NATIVE_HAWAIIAN_AND_OTHER_PACIFIC_ISLANDER', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t6 features in original data used to generate 6 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 8.59 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.33s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.01332665223834451, Train Rows: 185094, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.67s of the 299.67s of remaining time.\n",
      "\t-0.0287\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.04s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 295.42s of the 295.42s of remaining time.\n",
      "\t-0.0289\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.27s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 294.98s of the 294.98s of remaining time.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/axelsariel/repos/HousingDemographics/venv/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /Users/axelsariel/repos/HousingDemographics/venv/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: LightGBM ... Training model for up to 294.5s of the 294.5s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. If you are using Mac OSX, Please try 'brew install libomp'. Detailed info: dlopen(/Users/axelsariel/repos/HousingDemographics/venv/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so, 0x0006): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\n",
      "  Referenced from: <C3EB28DD-60B6-3334-AFA2-72BBBF9DBAEF> /Users/axelsariel/repos/HousingDemographics/venv/lib/python3.11/site-packages/lightgbm/lib/lib_lightgbm.so\n",
      "  Reason: tried: '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/opt/libomp/lib/libomp.dylib' (no such file), '/usr/local/lib/libomp.dylib' (no such file), '/usr/lib/libomp.dylib' (no such file, not in dyld cache)\n",
      "Fitting model: RandomForestMSE ... Training model for up to 294.38s of the 294.37s of remaining time.\n",
      "\t-0.028\t = Validation score   (-root_mean_squared_error)\n",
      "\t75.73s\t = Training   runtime\n",
      "\t0.11s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 214.71s of the 214.71s of remaining time.\n",
      "\tWarning: Exception caused CatBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 214.47s of the 214.47s of remaining time.\n",
      "\t-0.0271\t = Validation score   (-root_mean_squared_error)\n",
      "\t18.56s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 192.4s of the 192.4s of remaining time.\n",
      "\t-0.0265\t = Validation score   (-root_mean_squared_error)\n",
      "\t143.83s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 48.41s of the 48.41s of remaining time.\n",
      "\t-0.0265\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 47.43s of the 47.43s of remaining time.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\t-0.0265\t = Validation score   (-root_mean_squared_error)\n",
      "\t46.4s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.67s of the 1.0s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch': 0.579, 'XGBoost': 0.368, 'ExtraTreesMSE': 0.053}\n",
      "\t-0.0265\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 299.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240429_193316\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=label).fit(train_data, time_limit=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11553     0.037565\n",
       "1177      0.037621\n",
       "3111      0.038498\n",
       "111294    0.037847\n",
       "173138    0.037893\n",
       "Name: GrowthRate, dtype: float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predictor.predict(test_data.drop(columns=[label]))\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    62532.000000\n",
       "mean         0.037909\n",
       "std          0.000916\n",
       "min          0.036678\n",
       "25%          0.037361\n",
       "50%          0.037656\n",
       "75%          0.038147\n",
       "max          0.093136\n",
       "Name: GrowthRate, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root_mean_squared_error': -0.022863303691911217,\n",
       " 'mean_squared_error': -0.0005227306557085611,\n",
       " 'mean_absolute_error': -0.00468019658699008,\n",
       " 'r2': 0.0015289721288908042,\n",
       " 'pearsonr': 0.039112679777562984,\n",
       " 'median_absolute_error': -0.0031823569053061537}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(test_data, silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.022863</td>\n",
       "      <td>-0.026529</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.184967</td>\n",
       "      <td>0.124466</td>\n",
       "      <td>65.940737</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NeuralNetTorch</td>\n",
       "      <td>-0.022865</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.239714</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>46.397445</td>\n",
       "      <td>0.239714</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>46.397445</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NeuralNetFastAI</td>\n",
       "      <td>-0.022876</td>\n",
       "      <td>-0.026536</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.734077</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>143.829048</td>\n",
       "      <td>0.734077</td>\n",
       "      <td>0.047621</td>\n",
       "      <td>143.829048</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>-0.022884</td>\n",
       "      <td>-0.026535</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.022996</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.967800</td>\n",
       "      <td>0.022996</td>\n",
       "      <td>0.003954</td>\n",
       "      <td>0.967800</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ExtraTreesMSE</td>\n",
       "      <td>-0.023553</td>\n",
       "      <td>-0.027112</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.919179</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>18.563579</td>\n",
       "      <td>1.919179</td>\n",
       "      <td>0.104315</td>\n",
       "      <td>18.563579</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestMSE</td>\n",
       "      <td>-0.023966</td>\n",
       "      <td>-0.028009</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.342734</td>\n",
       "      <td>0.113152</td>\n",
       "      <td>75.729671</td>\n",
       "      <td>2.342734</td>\n",
       "      <td>0.113152</td>\n",
       "      <td>75.729671</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KNeighborsUnif</td>\n",
       "      <td>-0.024914</td>\n",
       "      <td>-0.028690</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.173192</td>\n",
       "      <td>0.137254</td>\n",
       "      <td>4.036143</td>\n",
       "      <td>2.173192</td>\n",
       "      <td>0.137254</td>\n",
       "      <td>4.036143</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>KNeighborsDist</td>\n",
       "      <td>-0.025859</td>\n",
       "      <td>-0.028923</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.210432</td>\n",
       "      <td>0.096603</td>\n",
       "      <td>0.271088</td>\n",
       "      <td>2.210432</td>\n",
       "      <td>0.096603</td>\n",
       "      <td>0.271088</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val              eval_metric  \\\n",
       "0  WeightedEnsemble_L2   -0.022863  -0.026529  root_mean_squared_error   \n",
       "1       NeuralNetTorch   -0.022865  -0.026536  root_mean_squared_error   \n",
       "2      NeuralNetFastAI   -0.022876  -0.026536  root_mean_squared_error   \n",
       "3              XGBoost   -0.022884  -0.026535  root_mean_squared_error   \n",
       "4        ExtraTreesMSE   -0.023553  -0.027112  root_mean_squared_error   \n",
       "5      RandomForestMSE   -0.023966  -0.028009  root_mean_squared_error   \n",
       "6       KNeighborsUnif   -0.024914  -0.028690  root_mean_squared_error   \n",
       "7       KNeighborsDist   -0.025859  -0.028923  root_mean_squared_error   \n",
       "\n",
       "   pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  \\\n",
       "0        2.184967       0.124466   65.940737                 0.003078   \n",
       "1        0.239714       0.015461   46.397445                 0.239714   \n",
       "2        0.734077       0.047621  143.829048                 0.734077   \n",
       "3        0.022996       0.003954    0.967800                 0.022996   \n",
       "4        1.919179       0.104315   18.563579                 1.919179   \n",
       "5        2.342734       0.113152   75.729671                 2.342734   \n",
       "6        2.173192       0.137254    4.036143                 2.173192   \n",
       "7        2.210432       0.096603    0.271088                 2.210432   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.000736           0.011912            2       True   \n",
       "1                0.015461          46.397445            1       True   \n",
       "2                0.047621         143.829048            1       True   \n",
       "3                0.003954           0.967800            1       True   \n",
       "4                0.104315          18.563579            1       True   \n",
       "5                0.113152          75.729671            1       True   \n",
       "6                0.137254           4.036143            1       True   \n",
       "7                0.096603           0.271088            1       True   \n",
       "\n",
       "   fit_order  \n",
       "0          8  \n",
       "1          7  \n",
       "2          5  \n",
       "3          6  \n",
       "4          4  \n",
       "5          3  \n",
       "6          1  \n",
       "7          2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 6 features using 5000 rows with 5 shuffle sets...\n",
      "\t38.28s\t= Expected runtime (7.66s per shuffle set)\n",
      "\t12.75s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WHITE</th>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HISPANIC_OR_LATINO</th>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BLACK_OR_AFRICAN_AMERICAN</th>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ASIAN</th>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AMERICAN_INDIAN_AND_ALASKA_NATIVE</th>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.012391</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>-0.000058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATIVE_HAWAIIAN_AND_OTHER_PACIFIC_ISLANDER</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            importance    stddev   p_value  n  \\\n",
       "WHITE                                         0.000425  0.000078  0.000127  5   \n",
       "HISPANIC_OR_LATINO                            0.000375  0.000108  0.000753  5   \n",
       "BLACK_OR_AFRICAN_AMERICAN                     0.000342  0.000073  0.000228  5   \n",
       "ASIAN                                         0.000309  0.000112  0.001771  5   \n",
       "AMERICAN_INDIAN_AND_ALASKA_NATIVE             0.000185  0.000118  0.012391  5   \n",
       "NATIVE_HAWAIIAN_AND_OTHER_PACIFIC_ISLANDER    0.000008  0.000003  0.001430  5   \n",
       "\n",
       "                                            p99_high   p99_low  \n",
       "WHITE                                       0.000585  0.000265  \n",
       "HISPANIC_OR_LATINO                          0.000598  0.000152  \n",
       "BLACK_OR_AFRICAN_AMERICAN                   0.000492  0.000193  \n",
       "ASIAN                                       0.000540  0.000078  \n",
       "AMERICAN_INDIAN_AND_ALASKA_NATIVE           0.000428 -0.000058  \n",
       "NATIVE_HAWAIIAN_AND_OTHER_PACIFIC_ISLANDER  0.000013  0.000002  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.feature_importance(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
